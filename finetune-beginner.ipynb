{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Mistral 7B Fine-Tuning with Professor Synapse Dataset\n",
    "This notebook implements a QLoRA fine-tuning workflow for Mistral 7B using Unsloth AI optimized training and the Professor Synapse conversational dataset[1][3][5]. Key features include 4096 token context handling, reasoning block integration, and Colab T4 GPU optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install datasets==2.18.0 trl==0.8.0 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastMistralModel\n",
    "\n",
    "model, tokenizer = FastMistralModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    token = \"hf_...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process-data"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SynapticLabs/professor-synapse\", split=\"train\")\n",
    "\n",
    "def format_conversation(example):\n",
    "    messages = example[\"messages\"]\n",
    "    formatted = \"<s>\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            formatted += \"[INST] \" + msg[\"content\"] + \" [/INST]\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            formatted += \"<reasoning>\" + msg.get(\"reasoning\", \"\") + \"</reasoning>\"\n",
    "            formatted += msg[\"content\"] + \"</s>\"\n",
    "    \n",
    "    return {\"text\": formatted}\n",
    "\n",
    "dataset = dataset.map(format_conversation, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-params"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 4096,\n",
    "    packing = True,\n",
    "    args = {\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"fp16\": not torch.cuda.is_bf16_supported(),\n",
    "        \"bf16\": torch.cuda.is_bf16_supported(),\n",
    "        \"logging_steps\": 50,\n",
    "        \"optim\": \"adamw_8bit\",\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"seed\": 42,\n",
    "        \"output_dir\": \"outputs\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"professor_synapse_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
