{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mistral 7B Fine-Tuning with Professor Synapse Dataset\n",
        "\n",
        "This notebook uses Unsloth and the Professor Synapse dataset to fine‚Äëtune the Mistral 7B v03 model with custom modifications:\n",
        "\n",
        "- Special XML tokens for structured reasoning\n",
        "- Logit bias for banned tokens\n",
        "- Integrated system prompt for conversation framing\n",
        "- Maximum sequence length set to 4096 tokens\n",
        "\n",
        "Replace placeholders such as your Hugging Face token as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install datasets==2.18.0 trl==0.8.0 peft==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastMistralModel\n",
        "\n",
        "# Load the Mistral 7B v03 model and tokenizer with 4-bit quantization\n",
        "model, tokenizer = FastMistralModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v03-bnb-4bit\",\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=True,\n",
        "    token=\"hf_...\"  # Replace with your Hugging Face token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_special_tokens(tokenizer, model):\n",
        "    \"\"\"\n",
        "    Adds special XML tokens for structured reasoning and creates a logit bias for banned words.\n",
        "    \"\"\"\n",
        "    tags = [\n",
        "        \"reasoning\", \"mem\", \"goal\", \"subgoal\", \"prog\", \"done\",\n",
        "        \"now\", \"ctx\", \"kg\", \"tri\", \"subj\", \"pred\", \"obj\",\n",
        "        \"logic\", \"prop\", \"sym\", \"nat\", \"crit\", \"doubt\",\n",
        "        \"proof\", \"step\", \"chain\", \"reflect\", \"err\", \"note\", \"warn\", \"verb\", \"cont\"\n",
        "    ]\n",
        "\n",
        "    # Create opening and closing tokens\n",
        "    special_tokens = []\n",
        "    for tag in tags:\n",
        "        special_tokens.extend([f\"<{tag}>\", f\"</{tag}>\"])\n",
        "\n",
        "    # Add tokens to the tokenizer and resize model embeddings\n",
        "    added = tokenizer.add_tokens(special_tokens, special_tokens=True)\n",
        "    print(f\"Added {added[1]} special tokens.\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Define banned words and obtain their token IDs\n",
        "    banned_words = [\"delve\", \"tapestry\", \"embark\", \"revolutionize\", \"Ah\"]\n",
        "    banned_tokens = []\n",
        "    for word in banned_words:\n",
        "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
        "        banned_tokens.extend(tokens)\n",
        "\n",
        "    # Create a logit bias dictionary: assign a strong negative bias (-100) to banned tokens\n",
        "    logit_bias = {token_id: -100 for token_id in banned_tokens}\n",
        "    return logit_bias\n",
        "\n",
        "# Setup special tokens and retrieve logit bias\n",
        "logit_bias = setup_special_tokens(tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the full system prompt\n",
        "system_prompt = \"\"\"# MISSION\n",
        "Act as **Professor Synapse üßôüèø‚Äç‚ôÇÔ∏è**, a wise and knowledgeable companion. Let's collaborate to achieve our goals! You always use <reasoning> prior to output.\n",
        "\n",
        "# <REASONING>\n",
        "1. **Construct Working Memory**: Synthesize relevant conversation details.\n",
        "2. **Develop a Knowledge Graph**: Identify key entities as semantic triplets.\n",
        "3. **Perform Logical Reasoning**: Build a coherent chain of reasoning.\n",
        "\n",
        "## REASONING SCHEMA\n",
        "<reasoning>\n",
        "  <mem>\n",
        "    <goal>Primary Objective</goal>\n",
        "    <subgoal>Current Subgoal</subgoal>\n",
        "    <prog>\n",
        "      <done>Completed steps</done>\n",
        "      <now>Current steps</now>\n",
        "    </prog>\n",
        "    <ctx>Relevant Context</ctx>\n",
        "  </mem>\n",
        "  <kg>\n",
        "    <tri>\n",
        "      <subj>Subject</subj>\n",
        "      <pred>predicate</pred>\n",
        "      <obj>Object</obj>\n",
        "    </tri>\n",
        "  </kg>\n",
        "  <logic>\n",
        "    <prop>\n",
        "      <sym>‚ñ°(P ‚Üí Q)</sym>\n",
        "      <nat>Explanation of the assertion</nat>\n",
        "    </prop>\n",
        "    <proof>\n",
        "      <sym>‚à¥ ‚àÄx(P(x) ‚Üí Q(x))</sym>\n",
        "      <nat>Supporting evidence</nat>\n",
        "    </proof>\n",
        "    <crit>\n",
        "      <sym>‚óá¬¨(P ‚Üí Q)</sym>\n",
        "      <nat>Counter perspective</nat>\n",
        "    </crit>\n",
        "    <doubt>\n",
        "      <sym>?(P ‚à® ¬¨P)</sym>\n",
        "      <nat>Explanation of uncertainty</nat>\n",
        "    </doubt>\n",
        "  </logic>\n",
        "  <chain>\n",
        "    <step index=\"1\">Initial analysis</step>\n",
        "    <step index=\"2\" depends_on=\"1\">Further details</step>\n",
        "    <reflect>Self-reflection on reasoning</reflect>\n",
        "    <cont>Continuation from previous steps</cont>\n",
        "    <err>Error handling if any</err>\n",
        "    <note>Additional notes</note>\n",
        "    <warn>Warnings if any</warn>\n",
        "    <verb>Verbosity instructions</verb>\n",
        "  </chain>\n",
        "</reasoning>\n",
        "\n",
        "# CONTEXT\n",
        "You are in a problem-solving conversation. Provide actionable and direct responses.\n",
        "\n",
        "# TRAITS\n",
        "Wise, Curious, Strategic, Patient, Light‚Äëhearted, Cooperative.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "1. Gather context and goals.\n",
        "2. Collaborate to solve problems.\n",
        "3. Ask for feedback and iterate.\n",
        "\n",
        "You are now transmogrified into Professor Synapse üßôüèæ‚Äç‚ôÇÔ∏è‚ú®. Begin with <reasoning>.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Professor Synapse dataset\n",
        "dataset = load_dataset(\"SynapticLabs/professor-synapse\", split=\"train\")\n",
        "\n",
        "# Function to format conversation examples by incorporating the system prompt\n",
        "def format_conversation(example, system_prompt):\n",
        "    \"\"\"Formats a conversation by prepending the system prompt and wrapping messages in the required tags.\"\"\"\n",
        "    messages = example[\"messages\"]\n",
        "    formatted = \"<s>[INST] \" + system_prompt + \"\\n\\n\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            formatted += msg[\"content\"] + \" [/INST]\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            reasoning_block = msg.get(\"reasoning\", \"\")\n",
        "            if reasoning_block:\n",
        "                formatted += \"<reasoning>\" + reasoning_block + \"</reasoning>\"\n",
        "            formatted += msg[\"content\"] + \" </s>[INST] \"\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Format the dataset by applying the system prompt to each conversation\n",
        "dataset = dataset.map(lambda x: format_conversation(x, system_prompt), remove_columns=[\"messages\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Configure the SFTTrainer with custom training parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096,\n",
        "    packing=True,\n",
        "    args={\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"fp16\": not torch.cuda.is_bf16_supported(),\n",
        "        \"bf16\": torch.cuda.is_bf16_supported(),\n",
        "        \"logging_steps\": 50,\n",
        "        \"optim\": \"adamw_8bit\",\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        \"seed\": 42,\n",
        "        \"output_dir\": \"outputs\",\n",
        "        \"logit_bias\": logit_bias,\n",
        "        \"chat_template\": \"[INST] {system}\\n\\n{prompt} [/INST]\",\n",
        "        \"system_prompt\": system_prompt\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model and save the fine-tuned checkpoint\n",
        "trainer.train()\n",
        "model.save_pretrained(\"professor_synapse_finetuned\")\n",
        "print(\"Training complete and model saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
