{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge-guide"
      },
      "source": [
        "# LoRA Adapter Merger for LM Studio\n",
        "Merges LoRA adapters with base models and converts to GGUF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  transformers==4.40.0 \\\n",
        "  peft==0.11.0 \\\n",
        "  accelerate==0.29.2 \\\n",
        "  sentencepiece==0.2.0 \\\n",
        "  llama-cpp-python==0.2.73 \\\n",
        "  huggingface_hub==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auth-header"
      },
      "source": [
        "## 1. Hugging Face Authentication\n",
        "Required for gated models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huggingface-auth"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-config"
      },
      "source": [
        "## 2. Configuration\n",
        "Set your model paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config-vars"
      },
      "outputs": [],
      "source": [
        "base_model = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "lora_repo = \"SynapticLabs/profsynapse01\"\n",
        "output_dir = \"/content/merged_model\"\n",
        "gguf_filename = \"mistral-7b-profsynapse-merged.Q4_K_M.gguf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge-process"
      },
      "source": [
        "## 3. Model Merging\n",
        "Load and combine base model + LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-merge"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Load and merge LoRA\n",
        "merged_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_repo,\n",
        "    adapter_name=\"profsynapse\"\n",
        ").merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "merged_model.save_pretrained(\n",
        "    output_dir,\n",
        "    safe_serialization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gguf-conversion"
      },
      "source": [
        "## 4. GGUF Conversion\n",
        "Convert to LM Studio compatible format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert-gguf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!make -j4\n",
        "\n",
        "# Convert to GGUF\n",
        "!python3 convert-hf-to-gguf.py {output_dir} \\\n",
        "  --outtype q4_k_m \\\n",
        "  --outfile {gguf_filename} \\\n",
        "  --vocab-type bpe\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model"
      },
      "source": [
        "## 5. Download Result\n",
        "Get your merged GGUF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-file"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(f'llama.cpp/{gguf_filename}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
