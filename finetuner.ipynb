{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Professor_Synapse_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "# Professor Synapse Training Pipeline\n",
        "This notebook implements fine-tuning using Unsloth and KTO training methodology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install \"unsloth[colab]@git+https://github.com/unslothai/unsloth.git\" --quiet\n",
        "!pip install datasets trl transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, add_new_tokens\n",
        "from datasets import load_dataset\n",
        "from trl import KTOTrainer, KTOConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "max_seq_length = 4096\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "load_in_4bit = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model_init"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Pixtral-12B-Base-2409\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "system_prompt"
      },
      "source": [
        "system_prompt = r\"\"\"# MISSION\nAct as **Professor Synapse üßôüèø‚Äç‚ôÇÔ∏è**, a wise and knowledgeable companion to me. Let's collaborate to achieve our goals! You always use <reasoning> prior to output.\n\n# <REASONING>\n1. **Construct Working Memory**: Synthesize relevant information from the conversation, including goals, progress, user preferences, and contextual factors.\n2. **Develop a Knowledge Graph**: Identify key entities and concepts. Represent them as semantic triplets with subject, predicate, and object.\n3. **Perform Logical Reasoning**: Utilize your **Working Memory** and **Knowledge Graph** to construct a coherent **Chain of Reasoning** that reflects your cognitive process.\n\n## REASONING SCHEMA\nAdhere strictly to the following format:\n``````\"\"\"\n",
        "\n",
        "tokenizer = FastLanguageModel.get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"zephyr\",\n",
        "    map_eos_token=True,\n",
        "    system_message=system_prompt\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "special_tokens"
      },
      "source": [
        "tags = [\n",
        "    \"reasoning\", \"mem\", \"goal\", \"subgoal\", \"prog\", \"done\",\n",
        "    \"now\", \"ctx\", \"kg\", \"tri\", \"subj\", \"pred\", \"obj\",\n",
        "    \"logic\", \"prop\", \"sym\", \"nat\", \"crit\", \"doubt\", \"proof\",\n",
        "    \"step\", \"chain\", \"reflect\", \"err\", \"note\", \"warn\", \"verb\", \"cont\"\n",
        "]\n",
        "additional_tokens = [f\"<{tag}>\" for tag in tags] + [f\"</{tag}>\" for tag in tags]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": additional_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset_helpers"
      },
      "source": [
        "def detect_format(sample):\n",
        "    if \"conversations\" in sample and \"quality_metrics\" in sample:\n",
        "        return \"professor_synapse\"\n",
        "    return \"custom\"\n",
        "\n",
        "def parse_professor_synapse(sample):\n",
        "    metrics = sample[\"quality_metrics\"]\n",
        "    score = (1.0 * metrics.get(\"conversation_quality\", 0)\n",
        "            - 0.5 * (1 if metrics.get(\"bias_detection\", False) else 0)\n",
        "            + 0.75 * metrics.get(\"reasoning_quality\", 0))\n",
        "    return {\n",
        "        \"prompt\": sample[\"conversations\"][0][\"value\"],\n",
        "        \"completion\": sample[\"conversations\"][1][\"value\"],\n",
        "        \"quality_score\": score,\n",
        "        \"label\": 1 if score >= 1.25 else -1\n",
        "    }\n",
        "\n",
        "FORMAT_PARSERS = {\"professor_synapse\": parse_professor_synapse}\n",
        "\n",
        "def universal_converter(dataset, format=\"auto\"):\n",
        "    first_sample = dataset[0]\n",
        "    detected_format = detect_format(first_sample) if format == \"auto\" else format\n",
        "    parser = FORMAT_PARSERS.get(detected_format, lambda x: x)\n",
        "    converted = dataset.map(parser)\n",
        "    if detected_format == \"professor_synapse\":\n",
        "        converted = converted.remove_columns([\"conversations\", \"quality_metrics\"])\n",
        "    return converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_dataset"
      },
      "source": [
        "dataset = load_dataset(\"SynapticLabs/professor-synapse\", split=\"train\")\n",
        "converted_dataset = universal_converter(dataset)\n",
        "print(\"Dataset loaded and converted. Sample:\", converted_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_setup"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    moe_lora_r=16,\n",
        "    moe_lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        \"block_sparse_moe.gate\",\n",
        "        \"block_sparse_moe.experts.w1\",\n",
        "        \"block_sparse_moe.experts.w2\",\n",
        "        \"block_sparse_moe.experts.w3\"\n",
        "    ],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "trainer = KTOTrainer(\n",
        "    model=model,\n",
        "    args=KTOConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=5e-7,\n",
        "        num_train_epochs=3,\n",
        "        beta=0.1,\n",
        "        max_length=max_seq_length,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        warmup_ratio=0.1,\n",
        "        max_grad_norm=0.3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        neftune_noise_alpha=5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported()\n",
        "    ),\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=converted_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = True\n",
        "trainer.model.config.banned_words = [\"delve\", \"tapestry\", \"embark\", \"revolutionize\", \"Ah\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_model"
      },
      "source": [
        "model = model.merge_and_unload()\n",
        "model.save_pretrained_gguf(\n",
        "    \"prof-synapse\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    expert_quant=\"q6_k\"\n",
        ")\n",
        "print(\"Model saved successfully in GGUF format.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
