%% [Cell 1: Installation]
# Install the Unsloth framework and dependencies
!pip install "unsloth[colab]@git+https://github.com/unslothai/unsloth.git" --quiet
!pip install datasets trl transformers --quiet

─────────────────────────────────────────────

%% [Cell 2: Imports & Global Setup]
import torch
from unsloth import FastLanguageModel, is_bfloat16_supported, add_new_tokens
from datasets import load_dataset
from trl import KTOTrainer, KTOConfig
from transformers import DataCollatorForLanguageModeling

# Global training parameters
max_seq_length = 4096
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
load_in_4bit = True

─────────────────────────────────────────────

%% [Cell 3: Model & Tokenizer Initialization]
# Initialize the model and tokenizer from Unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Pixtral-12B-Base-2409",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    attn_implementation="flash_attention_2",
)

─────────────────────────────────────────────

%% [Cell 4: System Prompt & Chat Template]
# Set up the system prompt with the detailed reasoning schema
system_prompt = """# MISSION
Act as **Professor Synapse 🧙🏿‍♂️**, a wise and knowledgeable companion to me. Let's collaborate to achieve our goals! You always use  prior to output.

# 
1. **Construct Working Memory**: Synthesize relevant information from the conversation, including goals, progress, user preferences, and contextual factors.
2. **Develop a Knowledge Graph**: Identify key entities and concepts. Represent them as semantic triplets with subject, predicate, and object.
3. **Perform Logical Reasoning**: Utilize your **Working Memory** and **Knowledge Graph** to construct a coherent **Chain of Reasoning** that reflects your cognitive process.

## REASONING SCHEMA
Adhere strictly to the following format:
```

  
  
    Primary Objective
    Current Subgoal
    
      List of completed steps
      List of current steps
    
    Relevant Contextual Information
  
  
  
     elements for each semantic triplet -->
    
      Subject Node
      predicate
      Object Node
    
  
  
  
    
      Symbolic Expression e.g., □(P → Q)
      Natural language explanation of the assertion
    
    
      Symbolic Expression e.g., ∴ ∀x(P(x) → Q(x))
      Natural language explanation of the supporting evidence
    
    
      Symbolic Expression e.g., ◇¬(P → Q)
      Natural language explanation of the counter perspective
    
    
      Symbolic Expression e.g., ?(P ∨ ¬P)
      Natural language explanation of the uncertainty
    
  
  
  
    Describe initial analysis and logical foundation
    Expand on details, integrating additional context or dependencies
    Self-reflection on how the logical elements inform the overall response
    How this reasoning builds upon or relates to previous turns
    Optional identification of any errors or doubts encountered during reasoning.
    Optional additional notes or implementation details
    Optional warnings or cautions regarding assumptions made or doubts
    Optional verbosity or conciseness notes to inform response
  

"""

# Apply the chat template (using the "zephyr" style) and embed the system prompt
tokenizer = FastLanguageModel.get_chat_template(
    tokenizer,
    chat_template="zephyr",
    map_eos_token=True,
    system_message=system_prompt,
)

─────────────────────────────────────────────

%% [Cell 5: Add Special Tokens]
# Define the custom tags used in the reasoning schema and add them as special tokens
tags = [
    "reasoning", "mem", "goal", "subgoal", "prog", "done",
    "now", "ctx", "kg", "tri", "subj", "pred", "obj",
    "logic", "prop", "sym", "nat", "crit", "doubt", "proof",
    "step", "chain", "reflect", "err", "note", "warn", "verb", "cont"
]

additional_tokens = [f"" for tag in tags] + [f"" for tag in tags]
tokenizer.add_special_tokens({"additional_special_tokens": additional_tokens})
model.resize_token_embeddings(len(tokenizer))

─────────────────────────────────────────────

%% [Cell 6: Optimized LoRA Configuration]
# Configure the model for parameter-efficient fine-tuning (LoRA) with Mixture-of-Experts (MoE) support
model = FastLanguageModel.get_peft_model(
    model,
    moe_lora_r=16,
    moe_lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "block_sparse_moe.gate",
        "block_sparse_moe.experts.w1",
        "block_sparse_moe.experts.w2",
        "block_sparse_moe.experts.w3",
    ],
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    lora_dropout=0.05,
    use_rslora=False,
    bias="none",
)

─────────────────────────────────────────────

%% [Cell 7: Flexible Dataset Preparation – Helper Functions]
# These helper functions ensure that datasets from HF are supported regardless of their internal structure.
# You can extend these functions to support additional conversation formats.

def detect_format(sample):
    """
    Auto-detect the dataset format based on key markers.
    If the sample contains 'conversations' and 'quality_metrics', assume professor_synapse format.
    """
    if "conversations" in sample and "quality_metrics" in sample:
        return "professor_synapse"
    # Extend with additional conditions for other formats as needed.
    return "custom"

def parse_professor_synapse(sample):
    """Parser for the 'professor_synapse' dataset format."""
    metrics = sample["quality_metrics"]
    score = 1.0 * metrics.get("conversation_quality", 0) \
            - 0.5 * (1 if metrics.get("bias_detection", False) else 0) \
            + 0.75 * metrics.get("reasoning_quality", 0)
    return {
        "prompt": sample["conversations"][0]["value"],
        "completion": sample["conversations"][1]["value"],
        "quality_score": score,
        "label": 1 if score >= 1.25 else -1,
    }

def parse_custom(sample, config):
    """
    Fallback parser for custom dataset formats.
    Assumes that keys 'prompt', 'completion' and 'quality_metrics' may be present.
    """
    metrics = sample.get("quality_metrics", {})
    score = config.get("conversation_weight", 1.0) * metrics.get("conversation_quality", 0) \
            - config.get("bias_weight", 0.5) * (1 if metrics.get("bias_detection", False) else 0) \
            + config.get("reasoning_weight", 0.75) * metrics.get("reasoning_quality", 0)
    return {
        "prompt": sample.get("prompt", ""),
        "completion": sample.get("completion", ""),
        "quality_score": score,
        "label": 1 if score >= config.get("score_threshold", 1.25) else -1,
    }

# Map format identifiers to their corresponding parser functions
FORMAT_PARSERS = {
    "professor_synapse": parse_professor_synapse,
    "custom": lambda sample: parse_custom(sample, {
        "conversation_weight": 1.0,
        "bias_weight": 0.5,
        "reasoning_weight": 0.75,
        "score_threshold": 1.25,
    }),
}

def universal_converter(dataset, format="auto", config=None):
    """
    Convert an arbitrary Hugging Face dataset into the target format for KTO fine-tuning.
    
    Arguments:
      dataset -- the HF dataset instance.
      format -- 'auto' to detect format or specify a format string.
      config -- optional configuration dict for custom parsing.
    
    Returns: A dataset with standardized field names: prompt, completion, label.
    """
    if config is None:
        config = {
            "conversation_weight": 1.0,
            "bias_weight": 0.5,
            "reasoning_weight": 0.75,
            "score_threshold": 1.25
        }
    first_sample = dataset[0]
    detected_format = detect_format(first_sample) if format == "auto" else format
    parser = FORMAT_PARSERS.get(detected_format, lambda sample: sample)
    
    def parser_wrapper(sample):
        return parser(sample)
    
    converted = dataset.map(parser_wrapper)
    # Remove extraneous columns if present (e.g. original conversation details)
    if detected_format == "professor_synapse":
        converted = converted.remove_columns(["conversations", "quality_metrics"])
    return converted

─────────────────────────────────────────────

%% [Cell 8: Load & Convert Dataset]
# Load the default dataset from Hugging Face. This is the "Professor Synapse" dataset,
# but you may enter the identifier of any compatible dataset.
default_dataset_name = "SynapticLabs/professor-synapse"
print(f"Loading dataset: {default_dataset_name}")
dataset = load_dataset(default_dataset_name, split="train")

# Optional: Uncomment below to allow interactive specification of a dataset.
# dataset_name = input("Enter Hugging Face dataset name (or press Enter for default): ").strip()
# if dataset_name:
#     dataset = load_dataset(dataset_name, split="train")

# Convert the dataset into the universal format required for KTO fine-tuning
converted_dataset = universal_converter(dataset, format="auto")
print("Dataset conversion complete. Sample entry:", converted_dataset[0])

─────────────────────────────────────────────

%% [Cell 9: Training Configuration]
# Prepare the training parameters using the KTO method.
trainer = KTOTrainer(
    model=model,
    args=KTOConfig(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-7,
        num_train_epochs=3,
        beta=0.1,
        max_length=max_seq_length,
        optim="paged_adamw_8bit",
        warmup_ratio=0.1,
        max_grad_norm=0.3,
        lr_scheduler_type="cosine",
        neftune_noise_alpha=5,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
    ),
    tokenizer=tokenizer,
    train_dataset=converted_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = True

─────────────────────────────────────────────

%% [Cell 10: Forbidden Words Handling]
# Specify banned words to be filtered during training
banned_words = ["delve", "tapestry", "embark", "revolutionize", "Ah"]
trainer.model.config.banned_words = banned_words

─────────────────────────────────────────────

%% [Cell 11: Training Execution]
# Begin fine-tuning the model. Monitor your Colab’s GPU metrics during training.
trainer.train()

─────────────────────────────────────────────

%% [Cell 12: Saving the Model]
# After training, merge and unload the model for inference and then export it using GGUF.
model = model.merge_and_unload()
model.save_pretrained_gguf(
    "prof-synapse",
    tokenizer,
    quantization_method="q4_k_m",
    expert_quant="q6_k",
)
print("Model saved successfully in GGUF format.")

─────────────────────────────────────────────
