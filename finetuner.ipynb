# %% [Installation Cell]
# [1] Install Unsloth with Colab dependencies
!pip install "unsloth[colab] @ git+https://github.com/unslothai/unsloth.git"
!pip install datasets trl

# Fix locale issues common in Colab [3][6]
import locale
locale.getpreferredencoding = lambda: "UTF-8"

# %% [Model Initialization]
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset

model_name = "unsloth/Pixtral-12B-Base-2409"  # From original code
max_seq_length = 4096  # Keep original setting
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
load_in_4bit = True  # Optimized for Colab T4 GPUs [4][10]

# [3][6] Optimized loading with Flash Attention
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    attn_implementation="flash_attention_2",
)

# %% [Chat Template Configuration]
# Original system prompt remains intact
system_prompt = """..."""  # Your full system prompt here

# [6] Official chat template method
tokenizer = FastLanguageModel.get_chat_template(
    tokenizer,
    chat_template="zephyr",
    map_eos_token=True,
    system_message=system_prompt,
)

# %% [Special Tokens Handling]
# [6] Use official add_new_tokens method
additional_tags = [f"<{tag}>" for tag in ["reasoning", "mem", "goal", ...]]  # Your tags
from unsloth import add_new_tokens
add_new_tokens(model, tokenizer, new_tokens=additional_tags)

# %% [LoRA Configuration]
# [4][10] Updated MoE parameters for Pixtral
model = FastLanguageModel.get_peft_model(
    model,
    moe_lora_r=16,
    moe_lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "block_sparse_moe.gate",
        "block_sparse_moe.experts.w1",
        "block_sparse_moe.experts.w2",
        "block_sparse_moe.experts.w3",
    ],
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    lora_dropout=0.05,
    bias="none",
)

# %% [Dataset Preparation]
# [7][9] KTO optimization from original code
def format_kto(example):
    metrics = example["quality_metrics"]
    score = (1.0 * metrics["conversation_quality"] 
             - 0.5 * metrics["bias_detection"] 
             + 0.75 * metrics["reasoning_quality"])
    return {
        "prompt": example["conversations"][0]["value"],
        "completion": example["conversations"][1]["value"],
        "label": 1 if score >= 1.25 else -1,
    }

# [10] Cloud storage integration
dataset = load_dataset("json", data_files="prof_synapse.jsonl", split="train")
dataset = dataset.map(format_kto).remove_columns(["conversations", "quality_metrics"])

# %% [Training Configuration]
from trl import KTOTrainer, KTOConfig

# [4][7] Updated KTO params for Pixtral
trainer = KTOTrainer(
    model=model,
    args=KTOConfig(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-7,
        num_train_epochs=3,
        beta=0.1,
        max_length=max_seq_length,
        optim="paged_adamw_8bit",
        warmup_ratio=0.1,
        max_grad_norm=0.3,
        lr_scheduler_type="cosine",
        neftune_noise_alpha=5,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
    ),
    tokenizer=tokenizer,
    train_dataset=dataset,
)

# %% [Banned Words Handling]
# [6] Official implementation
banned_words = ["delve", "tapestry", "embark", "revolutionize", "Ah"]
trainer.model.config.banned_words = banned_words

# %% [Training Execution]
# [3][10] Enable 2x faster inference
FastLanguageModel.for_inference(model)
trainer.train()

# %% [Model Saving]
# [6][10] GGUF export with Pixtral support
model.save_pretrained_gguf(
    "prof-synapse",
    tokenizer,
    quantization_method="q4_k_m",
    expert_quant="q6_k",
)

# [3] Optional TGI conversion
# model.save_pretrained_for_tgi("prof-synapse-tgi")
