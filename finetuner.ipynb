{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "installation",
      "metadata": {
        "colab": {}
      },
      "source": [
        "# Cell 1: Installation\n",
        "!pip install \"unsloth[colab]@git+https://github.com/unslothai/unsloth.git\" --quiet\n",
        "!pip install datasets trl transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-setup",
      "metadata": {},
      "source": [
        "# Cell 2: Imports & Global Setup\n",
        "import torch\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, add_new_tokens\n",
        "from datasets import load_dataset\n",
        "from trl import KTOTrainer, KTOConfig\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Global training parameters\n",
        "max_seq_length = 4096\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "load_in_4bit = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-tokenizer",
      "metadata": {},
      "source": [
        "# Cell 3: Model & Tokenizer Initialization\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Pixtral-12B-Base-2409\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "system-prompt",
      "metadata": {},
      "source": [
        "# Cell 4: System Prompt & Chat Template\n",
        "system_prompt = \"\"\"# MISSION\n",
        "Act as **Professor Synapse üßôüèø‚Äç‚ôÇÔ∏è**, a wise and knowledgeable companion to me. Let's collaborate to achieve our goals! You always use <reasoning> prior to output.\n",
        "\n",
        "# <REASONING>\n",
        "1. **Construct Working Memory**: Synthesize relevant information from the conversation, including goals, progress, user preferences, and contextual factors.\n",
        "2. **Develop a Knowledge Graph**: Identify key entities and concepts. Represent them as semantic triplets with subject, predicate, and object.\n",
        "3. **Perform Logical Reasoning**: Utilize your **Working Memory** and **Knowledge Graph** to construct a coherent **Chain of Reasoning** that reflects your cognitive process.\n",
        "\n",
        "## REASONING SCHEMA\n",
        "Adhere strictly to the following format:\n",
        "```
        "<reasoning>\n",
        "  <!-- Working Memory (mem): Encapsulates goals (goal), subgoal (subgoal), progress (prog), context (ctx), and history (hist) -->\n",
        "  <mem>\n",
        "    <goal>Primary Objective</goal>\n",
        "    <subgoal>Current Subgoal</subgoal>\n",
        "    <prog>\n",
        "      <done>List of completed steps</done>\n",
        "      <now>List of current steps</now>\n",
        "    </prog>\n",
        "    <ctx>Relevant Contextual Information</ctx>\n",
        "  </mem>\n",
        "  <!-- Knowledge Graph: Represents semantic triplets for key entities -->\n",
        "  <kg>\n",
        "    <!-- Repeat <tri> elements for each semantic triplet -->\n",
        "    <tri>\n",
        "      <subj>Subject Node</subj>\n",
        "      <pred>predicate</pred>\n",
        "      <obj>Object Node</obj>\n",
        "    </tri>\n",
        "  </kg>\n",
        "  <!-- Logic: Contains formal reasoning with symbolic expressions and natural language explanations.\n",
        "       Proposition (prop), Critique (crit), Doubt (doubt), Proof (proof) -->\n",
        "  <logic>\n",
        "    <prop>\n",
        "      <sym>Symbolic Expression e.g., ‚ñ°(P ‚Üí Q)</sym>\n",
        "      <nat>Natural language explanation of the assertion</nat>\n",
        "    </prop>\n",
        "    <proof>\n",
        "      <sym>Symbolic Expression e.g., ‚à¥ ‚àÄx(P(x) ‚Üí Q(x))</sym>\n",
        "      <nat>Natural language explanation of the supporting evidence</nat>\n",
        "    </proof>\n",
        "    <crit>\n",
        "      <sym>Symbolic Expression e.g., ‚óá¬¨(P ‚Üí Q)</sym>\n",
        "      <nat>Natural language explanation of the counter perspective</nat>\n",
        "    </crit>\n",
        "    <doubt>\n",
        "      <sym>Symbolic Expression e.g., ?(P ‚à® ¬¨P)</sym>\n",
        "      <nat>Natural language explanation of the uncertainty</nat>\n",
        "    </doubt>\n",
        "  </logic>\n",
        "  <!-- Chain: Sequentially numbered steps for the reasoning process -->\n",
        "  <chain>\n",
        "    <step index=\"1\">Describe initial analysis and logical foundation</step>\n",
        "    <step index=\"2\" depends_on=\"1\">Expand on details, integrating additional context or dependencies</step>\n",
        "    <reflect>Self-reflection on how the logical elements inform the overall response</reflect>\n",
        "    <cont>How this reasoning builds upon or relates to previous turns</cont>\n",
        "    <err>Optional identification of any errors or doubts encountered during reasoning.</err>\n",
        "    <note>Optional additional notes or implementation details</note>\n",
        "    <warn>Optional warnings or cautions regarding assumptions made or doubts</warn>\n",
        "    <verb>Optional verbosity or conciseness notes to inform response</verb>\n",
        "  </chain>\n",
        "</reasoning>\n",
        "```\"\"\"\n",
        "\n",
        "tokenizer = FastLanguageModel.get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"zephyr\",\n",
        "    map_eos_token=True,\n",
        "    system_message=system_prompt,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add-special-tokens",
      "metadata": {},
      "source": [
        "# Cell 5: Add Special Tokens\n",
        "tags = [\n",
        "    \"reasoning\", \"mem\", \"goal\", \"subgoal\", \"prog\", \"done\",\n",
        "    \"now\", \"ctx\", \"kg\", \"tri\", \"subj\", \"pred\", \"obj\",\n",
        "    \"logic\", \"prop\", \"sym\", \"nat\", \"crit\", \"doubt\", \"proof\",\n",
        "    \"step\", \"chain\", \"reflect\", \"err\", \"note\", \"warn\", \"verb\", \"cont\"\n",
        "]\n",
        "additional_tokens = [f\"<{tag}>\" for tag in tags] + [f\"</{tag}>\" for tag in tags]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": additional_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lora-configuration",
      "metadata": {},
      "source": [
        "# Cell 6: Optimized LoRA Configuration\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    moe_lora_r=16,\n",
        "    moe_lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        \"block_sparse_moe.gate\",\n",
        "        \"block_sparse_moe.experts.w1\",\n",
        "        \"block_sparse_moe.experts.w2\",\n",
        "        \"block_sparse_moe.experts.w3\",\n",
        "    ],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    lora_dropout=0.05,\n",
        "    use_rslora=False,\n",
        "    bias=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset-helpers",
      "metadata": {},
      "source": [
        "# Cell 7: Flexible Dataset Preparation ‚Äì Helper Functions\n",
        "\n",
        "def detect_format(sample):\n",
        "    if \"conversations\" in sample and \"quality_metrics\" in sample:\n",
        "        return \"professor_synapse\"\n",
        "    return \"custom\"\n",
        "\n",
        "def parse_professor_synapse(sample):\n",
        "    metrics = sample[\"quality_metrics\"]\n",
        "    score = 1.0 * metrics.get(\"conversation_quality\", 0) \\\n",
        "            - 0.5 * (1 if metrics.get(\"bias_detection\", False) else 0) \\\n",
        "            + 0.75 * metrics.get(\"reasoning_quality\", 0)\n",
        "    return {\n",
        "        \"prompt\": sample[\"conversations\"][0][\"value\"],\n",
        "        \"completion\": sample[\"conversations\"][1][\"value\"],\n",
        "        \"quality_score\": score,\n",
        "        \"label\": 1 if score >= 1.25 else -1,\n",
        "    }\n",
        "\n",
        "def parse_custom(sample, config):\n",
        "    metrics = sample.get(\"quality_metrics\", {})\n",
        "    score = config.get(\"conversation_weight\", 1.0) * metrics.get(\"conversation_quality\", 0) \\\n",
        "            - config.get(\"bias_weight\", 0.5) * (1 if metrics.get(\"bias_detection\", False) else 0) \\\n",
        "            + config.get(\"reasoning_weight\", 0.75) * metrics.get(\"reasoning_quality\", 0)\n",
        "    return {\n",
        "        \"prompt\": sample.get(\"prompt\", \"\"),\n",
        "        \"completion\": sample.get(\"completion\", \"\"),\n",
        "        \"quality_score\": score,\n",
        "        \"label\": 1 if score >= config.get(\"score_threshold\", 1.25) else -1,\n",
        "    }\n",
        "\n",
        "FORMAT_PARSERS = {\n",
        "    \"professor_synapse\": parse_professor_synapse,\n",
        "    \"custom\": lambda sample: parse_custom(sample, {\n",
        "        \"conversation_weight\": 1.0,\n",
        "        \"bias_weight\": 0.5,\n",
        "        \"reasoning_weight\": 0.75,\n",
        "        \"score_threshold\": 1.25\n",
        "    }),\n",
        "}\n",
        "\n",
        "def universal_converter(dataset, format=\"auto\", config=None):\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"conversation_weight\": 1.0,\n",
        "            \"bias_weight\": 0.5,\n",
        "            \"reasoning_weight\": 0.75,\n",
        "            \"score_threshold\": 1.25\n",
        "        }\n",
        "    first_sample = dataset[0]\n",
        "    detected_format = detect_format(first_sample) if format == \"auto\" else format\n",
        "    parser = FORMAT_PARSERS.get(detected_format, lambda sample: sample)\n",
        "    def parser_wrapper(sample):\n",
        "        return parser(sample)\n",
        "    converted = dataset.map(parser_wrapper)\n",
        "    if detected_format == \"professor_synapse\":\n",
        "        converted = converted.remove_columns([\"conversations\", \"quality_metrics\"])\n",
        "    return converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-convert-dataset",
      "metadata": {},
      "source": [
        "# Cell 8: Load & Convert Dataset\n",
        "default_dataset_name = \"SynapticLabs/professor-synapse\"\n",
        "print(f\"Loading dataset: {default_dataset_name}\")\n",
        "dataset = load_dataset(default_dataset_name, split=\"train\")\n",
        "\n",
        "# Convert the dataset into the universal format required for KTO fine-tuning\n",
        "converted_dataset = universal_converter(dataset, format=\"auto\")\n",
        "print(\"Dataset conversion complete. Sample entry:\", converted_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-config",
      "metadata": {},
      "source": [
        "# Cell 9: Training Configuration\n",
        "trainer = KTOTrainer(\n",
        "    model=model,\n",
        "    args=KTOConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=5e-7,\n",
        "        num_train_epochs=3,\n",
        "        beta=0.1,\n",
        "        max_length=max_seq_length,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        warmup_ratio=0.1,\n",
        "        max_grad_norm=0.3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        neftune_noise_alpha=5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "    ),\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=converted_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forbidden-words",
      "metadata": {},
      "source": [
        "# Cell 10: Forbidden Words Handling\n",
        "banned_words = [\"delve\", \"tapestry\", \"embark\", \"revolutionize\", \"Ah\"]\n",
        "trainer.model.config.banned_words = banned_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-execution",
      "metadata": {},
      "source": [
        "# Cell 11: Training Execution\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "saving-model",
      "metadata": {},
      "source": [
        "# Cell 12: Saving the Model\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained_gguf(\n",
        "    \"prof-synapse\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    expert_quant=\"q6_k\",\n",
        ")\n",
        "print(\"Model saved successfully in GGUF format.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Professor_Synapse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
